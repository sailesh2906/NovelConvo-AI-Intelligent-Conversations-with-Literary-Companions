{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import pysolr\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(paragraph):\n",
    "    text = paragraph.replace(\"'\", \"\")\n",
    "    text = text.replace('“',\"\")\n",
    "    text = text.replace('”',\"\")\n",
    "    text = text.replace('\"',\"\")\n",
    "    text = text.replace('_',\"\")\n",
    "    text = text.replace('—',\"\")\n",
    "    text = text.replace('-',\" \")\n",
    "    text = text.replace('\\n',\"\")\n",
    "    stop = set(nltk.corpus.stopwords.words('english')+ list(string.punctuation))\n",
    "    filtered_words = [i.lower().strip() for i in nltk.word_tokenize(text) if i not in stop]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]   \n",
    "    para = ' '.join(lemmatized_words)\n",
    "    return para"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the list of books in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_files = os.listdir(\"Books\")\n",
    "book_files.remove('.DS_Store')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dictionary with keys as title and author and values as paragraphs in the novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_para_dict = {}\n",
    "content_text = ['\\nContents','Contents','CONTENTS']\n",
    "for book in book_files:\n",
    "    with open(f'Books/{book}', 'r') as file:\n",
    "        book_split = file.read().split('\\n\\n')\n",
    "        title_line = next((line for line in book_split if line.startswith(\"Title:\")), None)\n",
    "        author_line = next((line for line in book_split if line.startswith(\"\\nAuthor:\")), None)\n",
    "        name = title_line.split(\"Title:\")[1].strip()\n",
    "        author = author_line.split(\"\\nAuthor:\")[1].strip()\n",
    "        book_split_clean = [pre_processing(ele) for ele in book_split] \n",
    "        start_line = next((line for line in book_split_clean if line.startswith(\"start of the project gutenberg ebook\")), None)\n",
    "        start_index = book_split_clean.index(start_line)\n",
    "        end_line = next((line for line in book_split_clean if line.startswith(\"end of the project gutenberg ebook\")), None)\n",
    "        end_index = book_split_clean.index(end_line)\n",
    "        paragraphs = [ele for ele in book_split_clean[start_index+1:end_index] if ele != \"\"]\n",
    "    book_para_dict[(name,author)] = paragraphs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the dictionary to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>para_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Adventures of Sherlock Holmes</td>\n",
       "      <td>Arthur Conan Doyle</td>\n",
       "      <td>the adventure sherlock holmes arthur conan doy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Adventures of Sherlock Holmes</td>\n",
       "      <td>Arthur Conan Doyle</td>\n",
       "      <td>wedlock suit remarked i think watson puton sev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Adventures of Sherlock Holmes</td>\n",
       "      <td>Arthur Conan Doyle</td>\n",
       "      <td>frequently how often well hundred time then ma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Adventures of Sherlock Holmes</td>\n",
       "      <td>Arthur Conan Doyle</td>\n",
       "      <td>i carefully examined writing paper upon waswri...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Adventures of Sherlock Holmes</td>\n",
       "      <td>Arthur Conan Doyle</td>\n",
       "      <td>a pair sound said yes continued glancing ofthe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title              author  \\\n",
       "0  The Adventures of Sherlock Holmes  Arthur Conan Doyle   \n",
       "1  The Adventures of Sherlock Holmes  Arthur Conan Doyle   \n",
       "2  The Adventures of Sherlock Holmes  Arthur Conan Doyle   \n",
       "3  The Adventures of Sherlock Holmes  Arthur Conan Doyle   \n",
       "4  The Adventures of Sherlock Holmes  Arthur Conan Doyle   \n",
       "\n",
       "                                           paragraph  para_id  \n",
       "0  the adventure sherlock holmes arthur conan doy...        0  \n",
       "1  wedlock suit remarked i think watson puton sev...        1  \n",
       "2  frequently how often well hundred time then ma...        2  \n",
       "3  i carefully examined writing paper upon waswri...        3  \n",
       "4  a pair sound said yes continued glancing ofthe...        4  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for (name, author), paragraphs in book_para_dict.items():\n",
    "    combined_para = []\n",
    "    for i in range(0, len(paragraphs),10):\n",
    "        combined_text = ' '.join(paragraphs[i:i+10])\n",
    "        combined_para.append(combined_text)\n",
    "    for paragraph in combined_para:\n",
    "        data.append({'title': name, 'author': author, 'paragraph': paragraph})\n",
    "\n",
    "books_df = pd.DataFrame(data)\n",
    "books_df['para_id'] = books_df.index\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Adventures of Sherlock Holmes' 'Romeo and Juliet' 'The Iliad'\n",
      " \"Gulliver's Travels into Several Remote Nations of the World\"\n",
      " 'Moby Dick; Or, The Whale' 'Against the Grain' 'Babbitt' 'Dracula'\n",
      " 'The Pilgrim Fathers of New England: a history' 'The Alchemist'\n",
      " 'Adventures of Huckleberry Finn' 'Hervey Willetts' 'The dark night'\n",
      " \"Janet's boys\"]\n"
     ]
    }
   ],
   "source": [
    "print(books_df['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.to_csv('novels_data.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLR indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_NAME = \"IRF23P3\"\n",
    "VM_IP = \"34.125.172.59\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_core(core=CORE_NAME):\n",
    "    print('sudo su - solr -c \"/opt/solr/bin/solr delete -c {core}\"'.format(core=core))\n",
    "\n",
    "\n",
    "def create_core(core=CORE_NAME):\n",
    "    print('sudo su - solr -c \"/opt/solr/bin/solr create -c {core} -n data_driven_schema_configs\"'.format(\n",
    "            core=core))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    def __init__(self):\n",
    "        self.solr_url = f'http://{VM_IP}:8983/solr/'\n",
    "        self.connection = pysolr.Solr(self.solr_url + CORE_NAME, always_commit=True, timeout=5000000)\n",
    "\n",
    "    def do_initial_setup(self):\n",
    "        delete_core()\n",
    "        create_core()\n",
    "\n",
    "    def create_documents(self, docs):\n",
    "        print(self.connection.add(docs))\n",
    "\n",
    "    def add_fields(self):\n",
    "        data = {\n",
    "            \"add-field\": [\n",
    "                {\n",
    "                    \"name\": \"title\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"indexed\": True,\n",
    "                    \"multiValued\": False\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"author\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"indexed\": True,\n",
    "                    \"multiValued\": False\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"paragraph\",\n",
    "                    \"type\": \"text_en\",\n",
    "                    \"indexed\": True,\n",
    "                    \"multiValued\": False\n",
    "                },\n",
    "                 {\n",
    "                    \"name\": \"para_id\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"indexed\": True,\n",
    "                    \"multiValued\": False\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        print(requests.post(self.solr_url + CORE_NAME + \"/schema\", json=data).json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo su - solr -c \"/opt/solr/bin/solr delete -c IRF23P3\"\n",
      "sudo su - solr -c \"/opt/solr/bin/solr create -c IRF23P3 -n data_driven_schema_configs\"\n"
     ]
    }
   ],
   "source": [
    "i = Indexer()\n",
    "i.do_initial_setup()\n",
    "# i.add_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'responseHeader': {'status': 0, 'QTime': 754}}\n"
     ]
    }
   ],
   "source": [
    "i.add_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":10676}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df = pd.read_csv('novels_data.csv')\n",
    "collection = books_df.to_dict('records')\n",
    "i.create_documents(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
